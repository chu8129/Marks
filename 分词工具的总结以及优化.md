### 分词标准
```
jieba/baidu:iob2
hanlp:ctb
```

---
### 关于hannlp-albert切词

```
原因：由于hanlpv2-large-albert的效率，想看看有没有优化手段；目前看来只能优化加载以及预测的过程，但过程逻辑比较精简，希望不大

关于分词：目前发现，几大分词，比较倾向hanlp
重点看了分词的tokenizer，
  使用的是公开的https://storage.googleapis.com/albert_models/albert_base_zh.tar.gz
  fit自己的预料，data/cws/large/all.txt
  参数耐心找会发现的～
```

```
代码中，各个模型的地址ALL，是运行时加载的，关键字:exec
```

```
加载large-albert流程
save_dir https://file.hankcs.com/hanlp/cws/large_cws_albert_base_20200828_011451.zip
metapath /Users/qw/.hanlp/cws/large_cws_albert_base_20200828_011451/meta.json
meta {'class_path': 'hanlp.components.tok.TransformerTokenizer', 'hanlp_version': '2.0.0-alpha.50'}
cls hanlp.components.tok.TransformerTokenizer
load config.json
>>>
```
---
### 关于hanlp的词性分析

```
这个关联fasttext的bin文件～～～好好下载吧，有点大，3G
>>> list(jieba.cut(line))
['你', '想到', '的', '是', '什么', '字', '呢', '？', '🤔', '#', '520', ' ', '#', '打出', '什么', '字', ' ', '#', '你', '想到', '了', '什么', ' ', '#', '解锁', '城市', '惊奇', ' ', '#', '安热', '沙美白', '防晒']
>>> list(model.cut(line))
['你', '想到', '的', '是', '什么', '字', '呢', '？', '🤔#520', ' ', '#', '打出', '什么', '字', ' ', '#', '你', '想到', '了', '什么', ' ', '#', '解锁', '城市', '惊奇', ' ', '#', '安热沙', '美白', '防晒']
>>> line
'你想到的是什么字呢？🤔#520 #打出什么字 #你想到了什么 #解锁城市惊奇 #安热沙美白防晒'
>>> tagger(model.cut(line))
['PN', 'VV', 'DEC', 'VC', 'PN', 'NN', 'SP', 'PU', 'CD', 'CD', 'PU', 'VV', 'PN', 'VV', 'CD', 'PU', 'PN', 'VV', 'AS', 'DT', 'CD', 'PU', 'VV', 'NN', 'NN', 'CD', 'PU', 'NR', 'NN', 'NN']
>>>
```

---

### 一些想法以及结论
#### 分词

```
总感觉百度的分词，比较依赖词性，例如喜茶、抖音、带货、直播间，但实事求是，总体覆盖，两个人两轮标注，百度最高(新媒体素材、硬广、微博数据)
hanlp，在人工抽查的错误集中表现良好(比较关注品牌以及热词)，由于是albert，qps。。。cpu，懂的

面壁反思了一下，目前的目标是，网络常用语尽量划分为单个且不会太长，故train-data为新媒体较好；
原则上pkuseg-crf会更合适，但从词性来说，百度没有问题，选择问题；
另外需要注意粗细粒度的控制，nlu说是有复合粒度

按leader的tips，百度会有长词出现
  个人猜测：可能原因是ner的问题，ccs采用的是ner结果非最短词(PER/LOC/ORG/TIME，注意百度的分词有两种包，lac和paddlehub，结果是不同的)，待测试

```
#### 词性
```
个人喜欢多个工具杂交，jieba和百度的词性稍微有点问题，转hanlp
```

### 关于腾讯nlu

```
nlu给出的结果比较良好，品牌有行业、相关信息，但也没有行业区分能力，需要自行剔除；
例子：车悬挂是独立悬挂，体验非常不错；@飘柔哥

关于名称蹭词，可规则剔除，如正则，快手会有账号结尾更为明显；官方账号可以从profile中提取已认证的结合榜单输出

```

---

### 百度和jieba对于喜茶分词的意外
```
# lac v2.2.0
+-----------------+----------------------------------------------------+
|   ModuleName    |lac                                                 |
+-----------------+----------------------------------------------------+
|     Version     |2.2.0                                               |
+-----------------+----------------------------------------------------+
|     Summary     |Baidu's open-source lexical analysis tool for Chin  |
|                 |ese, including word segmentation, part-of-speech t  |
|                 |agging & named entity recognition                   |
+-----------------+----------------------------------------------------+
|     Author      |baidu-nlp                                           |
+-----------------+----------------------------------------------------+
|  Author-Email   |paddle-dev@baidu.com                                |
+-----------------+----------------------------------------------------+
|    Location     |/Users/qw/.paddlehub/modules/lac                    |
+-----------------+----------------------------------------------------+

>>> model.cut(["29岁喜茶创始人聂云宸登上深圳富豪百人榜，我们一起来看看他的人生逆袭之路#喜茶#深圳#创业@抖音小助手"])
[{
'word': ['29岁', '喜茶', '创始人', '聂云宸', '登上', '深圳', '富豪', '百人', '榜', '，', '我们', '一起来', '看看', '他', '的', '人生', '逆袭', '之路', '#', '喜', '茶', '#', '深圳', '#', '创业', '@', '抖', '音', '小助手'], 
'tag': ['m', 'v', 'n', 'PER', 'v', 'LOC', 'n', 'n', 'n', 'w', 'r', 'd', 'v', 'r', 'u', 'n', 'v', 'n', 'w', 'v', 'n', 'w', 'LOC', 'w', 'vn', 'w', 'v', 'n', 'n']
}]
>>>

```

### 百度词性的一些区别
```
>>> line = "做出这个动作的都很厉害吧#厉害 #vans叠鞋挑战 #天生不同抖出自在 #奇闻奇事 #安热沙美白防晒"
>>> list(model.lexical_analysis([line]))
[{'word': ['做出', '这个', '动作', '的', '都', '很厉害', '吧', '#', '厉害', ' ', '#', 'vans', '叠鞋', '挑战', ' ', '#', '天生不同', '抖', '出自在', ' ', '#', '奇闻', '奇事', ' ', '#', '安热沙', '美白', '防晒'], 'tag': ['v', 'r', 'n', 'u', 'd', 'a', 'xc', 'xc', 'a', 'w', 'w', 'nz', 'n', 'v', 'w', 'w', 'ad', 'v', 'v', 'w', 'w', 'n', 'n', 'w', 'w', 'nr', 'v', 'vn']}]
>>> tagger(model.cut(line))
['VV', 'DT', 'NN', 'DEC', 'AD', 'VV', 'SP', 'PU', 'VV', 'CD', 'PU', 'NR', 'NN', 'NN', 'CD', 'PU', 'VV', 'PU', 'VV', 'CD', 'PU', 'NN', 'NN', 'CD', 'PU', 'NR', 'NN', 'NN']
>>> line = "你想到的是什么字呢？🤔#520 #打出什么字 #你想到了什么 #解锁城市惊奇 #安热沙美白防晒"
>>> tagger(model.cut(line))
['PN', 'VV', 'DEC', 'VC', 'PN', 'NN', 'SP', 'PU', 'CD', 'CD', 'PU', 'VV', 'PN', 'VV', 'CD', 'PU', 'PN', 'VV', 'AS', 'DT', 'CD', 'PU', 'VV', 'NN', 'NN', 'CD', 'PU', 'NR', 'NN', 'NN']
>>> list(model.lexical_analysis([line]))
[{'word': ['你', '想到', '的', '是', '什么', '字', '呢', '？', '🤔#520', ' ', '#', '打出', '什么', '字', ' ', '#', '你', '想到', '了', '什么', ' ', '#', '解锁', '城市', '惊奇', ' ', '#', '安热沙', '美白', '防晒'], 'tag': ['r', 'v', 'u', 'v', 'r', 'n', 'xc', 'w', 'nz', 'w', 'w', 'v', 'r', 'n', 'w', 'w', 'r', 'v', 'u', 'r', 'w', 'w', 'v', 'n', 'a', 'w', 'w', 'n', 'v', 'vn']}]
>>>
```

---

### 百度paddlehub的代码
```
1、ac自动机，是直接一个file，不用搜pya***，但在文档提示是 pip install ～～～
  使用lac模型时下载包内也有一个
2、user_dict，有时会成为你的一个坑，不太建议用，完美日记和完美/日记的效果，注意英文
  ac自动机的坑了解一下
3、py2时代的sdk，业务性质问题被baidu产品/销售/技术拉群，反馈过一次
  千万记得自己转gbk加ignore再转回来(内部转gbk，没有ignore，新媒体素材表情玩出天际，不知道改了没)
4、再来一个，paddlehub.Module(name='lac')，下载文件使用了tempfile.NamedTemporaryFile
  就没有自信跟hanlp一样明确给出从哪下载的并且保留一下或者给用户选择？要是大的文件想ci挂载。。还要看下代码
  统一度量衡的秦始皇真的有想法
5、读取模型文件版本时，咋一看还以为修饰器想喵喵操作，我勒个乖乖，字符串解析split，何必呢直接conf不是更直接
            module_info = re.findall('@moduleinfo\(.*?\)',
                                     file_content)[0].replace(
                                         '@moduleinfo(', '').replace(')', '')
            module_info = module_info.split(',')
            for item in module_info:
                if item.startswith('version'):
                    module_version = item.split('=')[1].replace(',', '')
```

---
### 计划

```
1 由于热词需要经常更新，模型也需要经常迭代，故需要经常更新分词并且是无监督类型，ccs拥有10^10的新媒体素材，计划自行跑一遍试试；
2 词性训练数据问题，各大工具有自己的训练库，这才是最宝贵的东西～；看看有没有参与的可能；
```

---

### 一些记录

|      函数      |                       结果                      |        import时间        |
|----------------|-------------------------------------------------|--------------------|
|   jieba_pack   |  雅姿/仙草/气色/锁/水面/膜/让/小/仙女们/美美/哒 | 2.0544190406799316 |
| baidu_lac_pack |   雅姿/仙草/气色/锁/水/面膜/让/小仙女们/美美哒  | 9.018001079559326  |
| baidu_hub_pack |  雅姿/仙草/气色/锁/水/面膜/让/小仙女们/美美/哒  | 1.924940824508667  |
|  pkuseg_pack   |  雅姿/仙草/气色/锁/水面/膜/让/小/仙女们/美美/哒 | 12.574051856994629 |
|  thulac_pack   |  雅姿仙/草气/色/锁/水面/膜/让/小仙女/们/美美/哒 | 2.621137857437134  |
|     ltp40      |  雅姿/仙草/气色/锁/水/面膜/让/小/仙女/们/美美哒 | 13.474219799041748 |
|   hanlp_pku    | 雅姿/仙草/气/色锁/水面膜/让/小/仙女/们/美/美/哒 | 2.388430118560791  |
|  hanlp_albert  |  雅姿/仙草/气色/锁水/面膜/让/小/仙女们/美美/哒  | 5.468364715576172  |


|      函数      |                                                               结果                                                               |        import时间        |
|----------------|----------------------------------------------------------------------------------------------------------------------------------|--------------------|
|   jieba_pack   |  29/岁/喜/茶/创始人/聂云宸/登上/深圳/富豪/百人/榜/，/我们/一/起来/看看/他/的/人生/逆袭/之路/#/喜茶/#/深圳/#/创业/@/抖音/小/助手  | 2.4759151935577393 |
| baidu_lac_pack |     29岁/喜/茶/创始人/聂云宸/登上/深圳/富豪/百人/榜/，/我们/一起来/看看/他/的/人生逆袭之路#喜茶#/深圳/#/创业/@/抖/音/小助手      |  9.42107105255127  |
| baidu_hub_pack |   29岁/喜茶/创始人/聂云宸/登上/深圳/富豪/百人/榜/，/我们/一起来/看看/他/的/人生/逆袭/之路/#/喜/茶/#/深圳/#/创业/@/抖/音/小助手   | 2.0757460594177246 |
|  pkuseg_pack   | 29/岁/喜/茶/创始人/聂云宸/登/上/深圳/富豪/百人榜/，/我们/一起/来/看看/他/的/人生/逆袭/之/路/#/喜茶/#/深圳/#/创业/@/抖音/小/助手  | 14.12120771408081  |
|  thulac_pack   | 29/岁/喜茶/创始人/聂云宸/登/上/深圳/富豪/百/人/榜/，/我们/一起/来/看看/他/的/人生/逆袭/之/路/#/喜茶/#/深圳/#/创业/@/抖音/小/助手 | 2.7273480892181396 |
|     ltp40      | 29/岁/喜茶/创始人/聂云宸/登/上/深圳/富豪/百人/榜/，/我们/一起/来/看看/他/的/人生/逆袭/之/路/#/喜茶/#/深圳/#/创业/@/抖音/小/助手  | 14.330671072006226 |
|   hanlp_pku    | 29/岁/喜茶/创始人/聂云宸/登上/深圳/富豪/百/人/榜/，/我们/一起/来/看看/他/的/人生/逆袭/之/路/#/喜茶/#/深圳/#/创业/@/抖音/小/助手  | 2.773092031478882  |
|  hanlp_albert  | 29/岁/喜茶/创始人/聂云宸/登上/深圳/富豪/百/人/榜/，/我们/一起/来/看看/他/的/人生/逆袭/之/路/#/喜茶/#/深圳/#/创业/@/抖音/小/助手  |  5.85502815246582  |





|      函数      |                                                                  结果                                                                 |        import时间        |
|----------------|---------------------------------------------------------------------------------------------------------------------------------------|--------------------|
|   jieba_pack   |   今天/带/大家/到/韩束/工厂/参观/下/，/明晚/20/:/00/常宝妈/直播间/韩束/护肤品/专场/等/你/来/#/带货/ /@/抖音/小/助手/@/DOU/+/小/助手   | 2.277297258377075  |
| baidu_lac_pack |    今天/带/大家/到/韩束/工厂/参观/下/，/明晚/20:00/常宝/妈/直播/间/韩束/护肤品/专场/等你/来/#/带/货/ /@/抖/音/小助手/@/DOU/+/小助手   | 9.158059120178223  |
| baidu_hub_pack |    今天/带/大家/到/韩束/工厂/参观/下/，/明晚20:00/常宝/妈/直播/间/韩束/护肤品/专场/等/你/来/#/带/货/ /@/抖/音/小助手/@/DOU/+/小助手   | 2.303596019744873  |
|  pkuseg_pack   |     今天/带/大家/到/韩束/工厂/参观/下/，/明晚/20:00/常/宝妈/直播间/韩束/护肤品/专场/等/你/来/#/带货/@/抖音/小/助手/@/DOU+/小/助手     | 12.207917928695679 |
|  thulac_pack   | 今天/带/大/家/到/韩束/工厂/参观/下/，/明晚/20/:/00/常/宝妈/直播/间/韩束/护肤品/专场/等/你/来/#/带/货/ /@/抖音/小/助手/@/DOU/+/小/助手 | 2.0897560119628906 |
|     ltp40      |    今天/带/大家/到/韩束/工厂/参观/下/，/明晚/20:00/常宝妈/直播间/韩束/护肤品/专场/等/你/来/#/带/货/@/抖/音/小/助手/@/DOU/+/小/助手    | 13.361646890640259 |
|   hanlp_pku    |    今天/带/大家/到/韩束/工厂/参观/下/，/明晚/20:00/常/宝妈/直播/间/韩束/护肤品/专场/等/你/来/#/带货/ /@/抖音/小/助手/@DOU+/小/助手    | 2.3146560192108154 |
|  hanlp_albert  |    今天/带/大家/到/韩束/工厂/参观/下/，/明晚/20:00/常/宝妈/直播间/韩束/护肤品/专场/等/你/来/#带货/ /@/抖音/小/助手/@/DOU/+/小/助手    | 5.318717956542969  |
